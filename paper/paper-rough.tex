\documentclass[12pt]{report}
\usepackage{setspace,mathtools,todonotes,indentfirst,biblatex}
\usepackage[margin=1in]{geometry}
\doublespacing
\addbibresource{paper.bib}
%\usepackage{helvet}
%\renewcommand*\familydefault{\sfdefault}
%\usepackage[T1]{fontenc}
\title{Terse: Creating a Programming Language}
\date{May 31, 2012}
\author{Robert Glossop}

\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\bs}{\textbackslash}

\begin{document}

\maketitle
\clearpage

\section*{Abstract}

Of all the problems in computer science, one of the most interesting
and challenging is creating a new programming language. It is one of
the oldest problems in computing, and one that is still being solved
today; new technology and new ideas constantly prompt people to try to
craft tools better than the previous generation's. Language design is
especially appealing to programmers because programming languages are
a tool they use every day; if they can improve the tool, they are
uniquely situated to benefit from their work. Finally, language design
is interesting in part because of a language's benefits beyond its
actual use: a well-written programming language can help people to
think about problems more clearly than they did before, even when
they're not using the language. 

Crafting a good language is not, however, simple. Language design,
even moreso than software in general, requires a deep knowledge both
of how people think and how machines work, and is in many ways as much
an art as a science. This paper examines the building blocks of
programming languages as well as the choices I made in designing my
own language, Terse.

\clearpage

\section*{On Languages in General}

Programming languages, both in design and implementation, have a great
deal of variety to them. To help describe this variety, several
criteria for categorizing a language have emerged.  How does its type
system function? What is its syntax like? Which programming paradigms
are best supported, and how permissive is it towards others? How
comprehensive is its standard library? While these by no means
describe a language in its entirety, they can help to get an idea of
the language's behavior and capabilities.

\subsection*{Programming Paradigms}

The most commonly used paradigms in modern programming languages are
imperative, object oriented and functional. Imperative programming is
essentially lists of instructions that get executed in order by the
computer.  These instruction lists are divided into functions, which
can pass data between each other throughout the course of the
program. This style is commonly used both because it makes intuitive
sense for the user\footnotemark{} and because, beneath all the
abstraction, this is how computers really operate. Thus, in the hands
of a skilled programmer, imperative languages offer very direct over
the machine. Some programmers, however, prefer languages that deal
directly in higher level logical entities such as mathematical
functions or objects; other language paradigms have been created to
explore this.

\footnotetext{This is not to say that imperative programming is
  necessarily the best suited for writing programs, merely that it
  maps most easily to the basic concept of writing a list of
  instructions for the program to execute.}

Object oriented programming is an expansion of imperative programming
in which data is packaged with the code that operates on it. The
results of this packaging are called objects, and they are meant to be
built to act analogously to objects in the real world. Proponents
claim that object oriented programming encourages code modularity and
that most problems can be greatly simplified by looking at them as
reactions between self-contained objects.  Opponents claim that many
problems are a poor fit for the object paradigm, and that forcing
people to stick to one method beyond its point of usefulness will only
lead to messy design. It's worth noting that object oriented design
can also be used, though not necessarily easily, in any procedural
language supporting function pointers.

Functional programming is a departure from the imperative style in
which side effects are theoretically not present: when a function is
called multiple times with the same value, it will always return the
same result, and it should not change anything else in the
process. The purpose of this limitation is to keep code simple,
modular and mathematically verifiable. Code in procedural languages
has the potential to grow very complex because of how procedures can
interact: namely, two apparently unrelated subroutines can influence
each others' behaviors in non-obvious ways simply by using the same
global state, and thus to understand the behavior of any subroutine
using a piece of global state one must have an idea of how this state
is modified by every other subroutine that touches it. By contrast,
while the larger purpose of a function without side effects may not be
immediately apparent, readers don't have to look beyond the single
function to deduce what it does. This perspective can prove to be a
great boon to complex software systems, though it can fall prey to the
same issues object orientation does: namely, not all problems can be
modeled well as strictly isolated, side effect free functions. ``Real
life has state,'' after all.

For Terse, I chose a simple procedural syntax as it is simple for both
humans and computers to understand, without unnecessarily limiting
programmers to a certain subset of the computer's capabilities. Much
of this is personal preference: I prefer languages that provide an
extensible core--even if they don't come with many specifics
implemented--over a language that tries to solve a lot of common
problems but doesn't give users adequate power to build on top of what
is already there.

\subsection*{Type Systems}

A persistent problem in programming languages is identifying what a
given piece of data represents. This ranges from the low level
(differentiating between a bit of text and a number in memory) to the
large scale (making sense of complex data structures). Programming
languages are divided by how they handle this problem into untyped,
weakly typed, and strongly typed languages. Untyped languages do not
keep track of data types at all, instead leaving this job entirely to
the programmer. This technique is not commonly used as it is almost
comically error prone even for small projects, and as the project
grows understanding or changing it quickly becomes impossible. Weakly
typed languages keep track of variable types but allow the programmer
considerable leeway in converting between types: multiplying
\code{"foo"} by three, for instance, may return \code{"foofoofoo"}, or
it may simply return the garbage result of trying to turn \code{"foo"}
into a number. Strongly typed systems track types strictly and do not
allow implicit conversion between them; the programmer must indicate
these conversions themselves. While this is ostensibly less error
prone, it also makes coding slower. Which system works best is largely
personal preference.

A separate but related concept to typing strength is static versus
dynamic typing. In static typing, the types of variables must be
stated explicitly by the programmer, while in dynamic typing, the
types of variables and arguments are discovered at runtime. Static
typing carries an advantage in speed: if a compiler knows exactly what
operations will be used based on the provided type declarations, it
can produce faster code than a compiler that must generate generic
code to be narrowed down at runtime. Static type declarations can also
aid programmers in figuring out the intended meaning of a program, as
well as to notify them when they're using data in a way that wasn't
intended. Dynamic typing, on the other hand, has an advantage in not
cluttering up code with information not relating to the logic itself,
as well as being able to more concisely describe generic interfaces.

Terse uses a strong, dynamic type system implemented mostly by
piggy backing off of its host language, Common Lisp. Converting
between types mostly has to be done manually with conversion functions
defined in the standard library. Terse will not provide optional type
declarations as their primary utility is in performance tuning, and
the interpreter is not nearly sophisticated enough to take advantage
of this.

\section*{Design}

Terse as a language is quite simple, consisting of very few built-in
statements and a minimal standard library. The language is based
around a design ``minimalism'' notably different from that found in
most software packages: where many systems will use minimalism to mean
a reduction to a few mathematically important building blocks, Terse's
minimalism is closer to ``what would be quickest to implement''. This
is acceptable given Terse's goal as an investigation of language
implementation; if it was meant for practical use, a very different
set of design goals would be called for.

\subsection*{Syntax}

Terse's syntax is primarily based around statements classified by
single-character identifiers and connected by the dot operator. All
statements evaluate to a value that can then be used in other
expressions, though that value can be nil. Variables are defined with
the letter \code{v}, the variable name, a dot, and its intended value,
eg. \code{vfoo.5} would declare a variable foo with the value
\code{5}. If statements are indicated with \code{i} and take a boolean
statement and a block to execute if the statement is true. Else
statements, when they occur, must come immediately after a preceding
if or else statement, and can either carry another boolean expression
or just a block body. While loops are written with \code{w} and a
conditional; the body will keep evaluating while the conditional is
true. In addition, loops in the style of C's for loops can be written
as \code{w.(init),(endcheck),(step) \bs{}block\bs{}}. As somewhat
alluded to in the previous example, statements can be parenthesized
within enclosing statements; this is sometimes necessary in order to
achieve the desired nesting effect.

Functions are defined with \code{f}, followed by the function's name,
an optional argument list, and its body. Blocks are denoted with
\code{\bs \bs}, with the packaged statements in between. The last
statement's return value will also serve as the value of the block,
which in the case of functions and control structures will be their
return values as well. Function names, as well as variable names, are
case sensitive. Functions are called with \code{c}, followed by a dot
and the function's arguments. Further arguments are separated by
commas. This leads to potential ambiguity with code like this:
\code{cfoo.cbar.1,2}. The argument \code{2} will obviously be passed
to either \code{foo} or \code{bar}, but which will it be? Terse's
solution is to parse and evaluate arguments right-associatively: both
\code{1} and \code{2} will thus be passed to \code{bar}, and the
result of this will then be fed into \code{foo}. This behavior can be
overridden, of course, by parenthesizing \code{cbar.1} to make it
clear that it is intended as a single statement.

Literals can be of integers, floating-point numbers, and strings, with
each of these behaving about as expected.  Array literals are written
with bracketed space-separated lists, which can be filled with either
literals or more dynamic expressions. Array values are accessed by
calling the array like a function. Strings are a specialization of
arrays containing only characters (themselves denoted with single
quotes). Arrays and strings are mutable; their values are settable by
using their index accesses as lvalues e.g. \code{v(carray.5).7}. Hash
tables can be created with the standard-library ``makehash'' function,
and accessed by calling the hash.

\subsection*{Standard Library}

The Terse standard library is fairly basic, consisting mostly of
functions for input, output and type conversion. IO targeting both the
console and files is supported. Network operations, GUI programming,
and most anything else are not. If I were to develop Terse into a real
language, a lot of additional functionality would be required, but as
it stands the below standard functions are sufficient.

\begin{table}
  \begin{tabular}{ | l | l | }
    \hline
    Function & Explanation \\
    \hline \hline
    \code{print.msg}				& Prints text out to the console		\\ \hline
    \code{println.msg}				& Message, plus a newline				\\ \hline
    \code{readchar.msg}				& Reads a character of text from input	\\ \hline
    \code{openfile.filename}		& Opens a file							\\ \hline
    \code{readfilechar.filename}	& Reads a character from the given file	\\ \hline
    \code{readfileline.filename}	& Likewise by line						\\ \hline
    \code{writefile.text,filename}	& Writes the given text to the file		\\ \hline
    \code{floor.num}				& Rounds a number down					\\ \hline
    \code{ceil.num}					& Rounds a number up					\\ \hline
    \code{round.num}				& Rounds a number						\\ \hline
    \code{makehash}					& Creates a hash table					\\ \hline
  \end{tabular}
  \caption{Terse builtin functions}
\end{table}

Much of the overhead of creating a new language\footnotemark{} is in
writing a complete, robust standard library to allow programmers to
focus more on their specific problem than the general ones along the
way. Languages with a specific focus will design their standard
library to support it, and general-purpose languages usually make
their standard libraries extensive enough that they can stand without
third-party support. Terse, as a purely academic language, doesn't
require these additions. Its standard library requirement starts and
ends at providing just enough interactivity to verify that the program
is doing the right thing, and that requirement is met.

\footnotetext{Up until the 1990s programming languages were not
  expected to have comprehensive standard libraries the way they are
  today; back then including things like advanced IO and regular
  expressions in the language itself would have been enough to
  consider the language bloated. Now it's simply expected.}

\section*{Implementation}

The Terse interpreter is, as far as language implementations go, quite
small and lightweight, largely by virtue of being hosted by a system
that already implements a robust type system, first-class functions,
and several other features that make language implementation
comparatively easy. The interpreter is written in Common Lisp and
weighs in at about 600 lines of code. It is split into two components:
the parser, which translates text from a file into something usable by
the program; and the runtime, which assembles this parse tree into
nested function calls that, when run, execute the target program. This
split structure is fairly ubiquitous in real-world programming
languages, though in my case both the language's design and
implementation are radically simpler than what is found even in the
most minimalist real-world language implementations.

\subsection*{The Parser}

The purpose of the parser is to transform a textual program into a
tree readable by the interpreter. It does this by reading the program
text in and, statement by statement, outputting the program's
equivalent tree form. This parse tree is then handed off to the
runtime to interpret as it sees fit; this division of labor makes both
parts much easier to deal with than if the interpreter proper was
parsing text on the go. This is the only real ``subsystem division''
Terse has.

Parsing for most languages takes place in two steps: the lexer and the
parser proper. The lexer splits the text stream into a sequential list
of tokens, and the parser interprets these to build a tree
representative of the program's structure. Languages which rely
significantly on context to determine a statement's meaning will
commonly use a lexer to help deal with the potential ambiguity. There
are even lexer generators available that will automatically generate
lexing code given a language description, such as GNU
Flex\cite{flex}. In Terse's case, however, the grammar is unambiguous
and easy to process--I designed the language emphasizing this over
readability--so a separate lexer, much less a lexer generator, would
be overkill.

The parser exists to take a linear program input (be it a lexer output
or plaintext) and turns it into an Abstract Syntax Tree--a recursive
structure more representative of the meaning of the program, and more
suited to processing by a compiler or interpreter. They work by
grabbing one token at a time from the input stream while maintaining a
program state to determine where in the tree the token will be
put. For example, a parser for a calculator program would transfer the
sequence $5 + 3 * 2 + 4$ into \code{[+ 5 [+ [* 3 2] 4]]} taking into
account order of operations. Other statement types are likewise
transformed, and the end result is a recursive list that can be
transformed into a runnable program by a subsequent compiler or
interpreter.

Like the lexing step, there are programs available that will
automatically generate a parser given a language definition; many
parser programs are designed to be easily chainable with an automated
lexing step (GNU Bison\cite{bison}, for example, plays nicely with
both Flex and Lex). These are especially useful for programming
languages that rely heavily on context for a token's meaning. A C-like
grammar, for example, needs to differentiate between a type
declaration, a function declaration and a function call without
differentiators in the token itself, and parser generators have
generally evolved to suit this need. Terse, however, is designed to be
free of such ambiguities, so an industrial grade parser generator
isn't necessary. There are also no major parser generators targeted at
Terse's host language, and creating bridging code would amount to a
significant project in itself. It was thus easiest to implement a
parser myself.

\begin{figure}
\begin{verbatim}
(defun parse (stream)
  "Dispatch function for all the other parsing funcs"
  (parse-whitespace stream)
  (let ((char (peek-char nil stream nil nil)))
    (case char
         (#\f (parse-defun stream))
         (#\v (parse-variable stream))
         (#\c (parse-funcall stream))
         (#\i (parse-if-statement stream))
         (#\w (parse-while-statement stream))
         (#\" (parse-string stream))
         (#\[ (parse-array stream))
         (#\( (parse-parened stream))
         (t
          (cond
            ((eql char nil) nil)
            ((digit-char-p char)
             (parse-number stream))
            (t (error (format nil "Parse failed at line ~a: ~a"
                              *current-line* char))))))))
\end{verbatim}
\caption{Source code for Terse's parser dispatch}
\end{figure}

The Terse parser is centered around a single top-level function,
\code{PARSE}, which dispatches to other parsing functions based on the
first character of the statement. The delegate function then returns a
tree representation of the target statement, calling \code{PARSE}
recursively on its components as appropriate. The structure of the
delegate functions is generally to read the selector character off the
stream, parse leading whitespace, and create a list of whatever
properties the expression has. Parsing a function call, for example,
involves reading the target function's name off the input stream and
calling \code{PARSE} on any arguments it is called with.

\begin{figure}
\begin{verbatim}
(defun parse-funcall (stream)
  "Parses a function call and its arguments"
  (read-char stream) (parse-whitespace stream)
  `(:funcall (:name . ,(eat-name stream))
             ,@(when (has-argument stream)
                     `((:args . ,(parse-arguments stream))))))
\end{verbatim}
\caption{The parsing function for Terse function calls}
\end{figure}

The output of \code{PARSE-FUNCALL} when called with
\code{cprint."Hello World!"} would be \code{(:funcall (:name
  . “print”) (:args "Hello World!"))}. Other parsing functions vary in
specifics, but their operations are basically the same. Running
multiple statements is specified by \code{(:progn exprs...)} and is
used both in block structures and in the file's top level. The file is
processed by simply calling \code{PARSE} until the parser hits
end-of-file, at which point the function returns nil and the completed
Abstract Syntax Tree is passed to the interpreter for running.

The parser is not without its bits of messiness; there are several
components that could do with some improving. The dispatch function,
for example, is currently a static list but would be better off as a
more automatic dispatch mechanism, similar to the selector I used in
the runtime component. Though the example I chose uses a list literal
and inserts the relevant code from there, most of the other functions
build the lists manually, a messy and unnecessary technique. Lastly,
\code{PARSE-BLOCK} should be reachable from the main dispatch
function, not called specially by the control structures. this would
allow additional lexical scopes to be inserted anywhere, not just
inside control structures.

\subsection*{The Runtime}

The Terse runtime consists of functions to transform the Abstract
Syntax Tree into something runnable, manipulate the symbol table and
deal with Terse values. The transformation consists of a top-level
function \code{COMPILE} which returns another function that, when
called, executes the program. Symbol lookup is handled by a series of
symbol tables, each containing a parent attribute; this provides a
function's lexical scoping. Terse values are stored as a thin wrapper
over Lisp's own type system, and are needed to provide for setting
array values and the like. Together, these provide all the necessary
elements to run a Terse program.

The symbol table implementation is fairly simple. It uses a hash table
bound to a global variable that stores string-indexed names, as well
as an optional parent slot. When a symbol is looked up, the lookup
function checks each level for a matching symbol name, returning the
first one it finds. The setter function works similarly, except that
it creates a new binding in the innermost level if an existing binding
isn't found. Symbol levels are currently added on function calls,
though it would probably be cleaner to add them to any block
structure.

Terse's data storage is similarly simple. Normal values are contained
within an instance of \code{VALUE}, which on its own is just a thin
wrapper for the data. Array values are represented as a subclass of
\code{VALUE}, which is necessary to allow it to be ``called'' to
access its indices. Array subscripts are represented by a different
\code{VALUE} subclass which allows them to be used as lvalues (values
on the left side of an assignment operator; assignment target).

Terse's interpretation step relies on generating functions to execute
each piece of the AST. This is done through the \code{COMPILE}
function. \code{COMPILE} itself is a dispatch function that selects an
applicable transformation based on the first item of the AST. Each of
its delegates transforms its target into a lambda expression, which
when called executes the intended statement and returns the desired
value. The compiled forms of literals are simply lambdas that return
their packed values, while the more complex statements perform
variable lookups, conditional execution or the like.

\begin{figure}
\begin{verbatim}
(defexpr :funcall stmt
  (let ((args (mapcar #'compile (get-property :args stmt))))
    (lambda ()
      (with-symbol-level
        (let ((symbol (lookup-symbol (get-property :name stmt)))
              (args (mapcar #'funcall args)))
          (if (typep symbol 'value)
              (call-value symbol args)
              (apply symbol args)))))))
\end{verbatim}
\caption{The transformation function for Terse function calls}
\end{figure}

This system is slow and lends itself poorly to optimization, but I
went with it anyways because it offers an exceptionally clean
implementation: each AST expression can be transformed one-to-one into
a ``compiled'' expression and executed. This also means that
expression nesting translates cleanly--an important consideration,
since expressions can be nested arbitrarily deeply. Perhaps most
importantly, it keeps the base language readily accessible: more
complicated code generation techniques would involve a specially-made
bridge to the host language--if it was available at all--whereas in
this native closure technique the base runtime is always close at
hand. This was a major factor in keeping the language implementation
as short and simple as it is.

Since processed Terse functions are essentially interchangable with
Lisp's own functions, builtin functions can be provided to Terse
simply by assigning the desired Lisp function to Terse's symbol
table. Supporting new datatypes is even easier: all that is needed are
the functions to create and manipulate them, Lisp's dynamic type
system takes care of the rest. A more extensive standard library
implementation would ideally use definitions written in Terse for many
of the functions, but as the language stands now that would only
overcomplicate things. More generally, a mechanism to load other
source files into a running program could and should be included, but
isn't yet.

\section*{Conclusion}

I chose to create a programming language to get an idea of the
challenges involved with making one. In this, I succeeded: for all of
Terse's weaknesses, it served as an effective demonstration of what's
necessary to make a programming language work. It's not suited for
real-world development, but it was never meant to be. At its primary
purpose--to show what's involved in creating a programming
language--it succeeded beautifully. It has provided me with experience
and insight that will be invaluable for writing programs in the
future, and especially in crafting programming languages.

What if I were to continue with Terse, though? What features would I
add to it to turn it into a useful language? The first thing would be
to fix closure support. A closure is a nested function definition that
has access to variables in its enclosing scope, and can use these
variables even after the outer function exits. Closures are an
incredibly useful feature found widely in scripting and other
high-level languages but rarely in the systems languages that are used
for the majority of today's software
development\footnotemark{}. Closures are also very easy to implement
in a language structured as Terse is: as every namespace is a distinct
hash table, it is trivial to create a new symbol table with each
function invocation, and to cache the current one with each function
definition. With this simple addition, Terse would suddenly have a
distinct advantage over even modern systems programming languages.

\footnotetext{C and its ilk traditionally use structs and classes to
  pass data between functions, whereas a closure is a nested function
  referencing its outer function environment. The choice between them
  is a polarizing issue, and both solutions have their applications;
  For most things, I prefer closures.}

Improving the standard library is also essential. As it stands now,
the standard library is only barely adequate for even simple file IO,
and is basically unusable for anything beyond that. Beyond better file
manipulation, the standard library should also include high-level
control constructs like \code{map} and \code{fold} that are commonly
included in other high level languages. While these are easy enough
for others to implement, they are used widely enough that it makes
sense for languages to include them. A foreign function
interface\footnotemark{} would also be good to have, though
implementing it would be a challenge.

\footnotetext{Foreign Function Interface: an interface that allows
  programs in one language to access code in another; the other is
  usually C.}

A truly nifty feature would be the addition of a macro system. Macros
are basically a textual subsitution facility used to replace every
instance of one identifier with another. In C and its derivatives they
have a reputation for being messy and difficult to debug, but in
languages with well-done macro mechanisms like Lisp they add an
incredible amount of flexibility to the language. Part of what makes
Lisp's macro system so successful Lisp's syntax, which is regular
enough to make macro processing easy and its use indistinguishable
from language builtins. Doing the same with Terse might be difficult,
but its uniform syntax makes the task easier than it would be in a
more traditional language.

Terse needs a good deal of development before it could be used for
real programming. Even if it was developed further, its base would
likely prove poorly suited to real use. Even so, writing the language,
and finding out the workings behind real world language
implementations, was an interesting experience that I look forward to
using in the future.

\clearpage

\printbibliography

The source code for Terse is available at
\code{http://correnos.dyndns.org/\textasciitilde{}correnos/terse.git}.

\end{document}
